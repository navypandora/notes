HADOOP

Make datanode and tasktracker on the same system (no delay in data transfer)
Tasktracker directly reads the data from the datanode


sp.getouput("hadoop dfsadmin -report | grep Name")
iplist = ip.list("\n")
for x in iplist:
	x.split(":").strip(-)

cat /etc/passwd | ./mapper.py | ./reducer.py

Hadoop just provides resources | MapReduce just helps us in Analysing data
1 Tasktracker has 2 max map and 2 max reduce task by default
